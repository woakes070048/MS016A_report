\chapter{A benchmarking tool based on httperf}

\textbf{Keywords: Webservices, performance, benchmarks }

\subsection*{Abstract}

In this report, a benchmark tool is developed, based on the well-known httperf tool for webserver performance measurements. 

\section{Introduction}

\subsubsection{Problem statement}

The given problem statement in this project was as follows:

\emph{Create a benchmarking tool, based on httperf, which is able to increase the request rate and gather data about a webservers performance }

\section{Background}

In order to fully comprehend the needed features of the benchmark tool, a short introduction to webserver performance and httperf is needed. 

\subsection{Benchmarking webservers}

A webservers job is to return web pages as quickly as possible. For a small number if clients, all webservers work pretty well. The important aspect is at which \emph{connection rate}, that is the number of requests per second, the webserver will show a reduction in performance, meaning it is has become saturated. At that point, additional clients will start to time out and receive errors. From a production point of view, this is an highly undesirable situation. Identifying the cutoff point is therefore important in order to know the limits of your service and make needed adjustments if necessary.\\

This cutoff point ( or saturation point) will show itself in several observable variables. One variable to look at is how fast a requested web-page is returned to a client, the \emph{connection time}. This variable will most likely change as one increases the number of clients. More pressure will result in a longer processing time for each request. However, this variable will not be equal for every client, since not every client requests the exact same page. One should therefore also expect a certain variation in this variable. With a high number of clients, one can get a good impression of the performance using the mean and median of the connection time. If the mean and median rise, one can conclude that the overall performance is dropping.\\

Another informative variable is the amount of errors, especially timeouts, are currently experienced by the clients. How many are experiencing bad service? This timeout is usually a fixed value, like 5 seconds. If the page is not returned within that limit, the client gives up and reports an error. Compared to the average connection time, this variable shows a different behavior. It will not rise slowly, as the pressure mounts, instead, it will remain zero - no errors - until the cutoff point is found. Then, suddenly, it will explode and jump up, showing the devastating effect a saturated webserver has. So this variable, although accurate in finding the cutoff point, will not give any signs of trouble before it, literally, is too late.\\

The methodology for finding the individual cutoff point for a site is to hammer the webserver with increasingly higher request rates until it folds. A common tool for this task is httperf, which will be discussed next.

\subsection{httperf}

The tool \Verb@httperf@ is a well established tool for benchmarking webpages. It imitates a given number of clients who access the same website. Since it is executed on a separate machine as the webserver (see Figure \ref{httperf}), it can be used to benchmark any type of webserver on any platform. One benefit of httperf is the number of options available to specify exactly how httperf should interact with the webserver.

\begin{figure}[ht!]
\begin{centering}
\includegraphics[width=12cm]{httperf_principle.pdf}
\caption{ The httperf command is executed from a client machine towards a webserver. }
\label{httperf}
\end{centering}
\end{figure}

Here follows a list of the most notable options for httperf. More information can be found by executing the command \Verb@man httperf@

\begin{itemize}
	\item The rate of connections per second
	\item The number of HTTP requests per connection
	\item The total number of connections
	\item The number of seconds considered to be acceptable
\end{itemize}

The most important option is the rate. However, without setting the other options correct too, one will not be able to make httperf function as desired. Here is one example:\\

We want to connect at a rate of 20 connections per second. The total number of connections is set to 100. The number of requests is 3 per connection. With this combination, the test will last for 5 seconds ( 100 connections at 20 per second ) and amount to a total of 300 requests. The problem with this combination is that 5 seconds is not a particularly long time and may not be enough to actually press the webserver. The same rate for a longer period, say minutes, would be more appropriate. Moreover, if the command is repeated with a rate of 50 and everything else equal, then the test would only last for 2 seconds!\\

It makes sense to think of the number of connections as "fuel" and the rate as the speed (or fuel consumption). More fuel means longer travel. So if you increase the rate, but want the test to last for as long as before, you need to increase the number of connections accordingly.\\

The next is an example execution of httperf (some text has been omitted for readability):

\begin{Verbatim}[commandchars=\\\{\},numbers=left,label=Httperf execution example]
httperf --server monitor.vlab.iu.hio.no --uri /munin --num-con  200 --rate 10 --timeout 5 --num-call 3

Total: connections 200 requests 600 replies 600 test-duration 19.902 s

Connection rate: 10.0 conn/s (99.5 ms/conn, <=1 concurrent connections)
\textcolor{cyan}{Connection time [ms]: min 2.0 avg 2.2 max 2.8 median 2.5 stddev 0.1}
Connection time [ms]: connect 0.4
Connection length [replies/conn]: 3.000

Request rate: 30.1 req/s (33.2 ms/req)
Request size [B]: 80.0

Reply rate [replies/s]: min 30.0 avg 30.0 max 30.0 stddev 0.0 (3 samples)
Reply time [ms]: response 0.6 transfer 0.0
Reply size [B]: header 267.0 content 359.0 footer 0.0 (total 626.0)
Reply status: 1xx=0 2xx=0 3xx=600 4xx=0 5xx=0

CPU time [s]: user 4.37 system 15.52 (user 21.9% system 78.0% total 99.9%)
Net I/O: 20.8 KB/s (0.2*10^6 bps)

\textcolor{cyan}{Errors: total 0 client-timo 0 socket-timo 0 connrefused 0 connreset 0}
Errors: fd-unavail 0 addrunavail 0 ftab-full 0 other 0
\end{Verbatim}
As we can see from the output, httperf displays a range of summary variables of the performance of the test. Let's first cover the input variables. The test was targeted at the webpage \Verb@http://monitor.vlab.iu.hio.no/munin@ (see options \Verb@--server@ and \Verb@--uri@ on line 1). The other options concern the level of pressure. A total of 200 connections at a rate of 10 per second. This will make the test take a total of 20 seconds. A request rate of 3 means a total of 600 http requests are issued. The timeout value is 5 seconds, meaning that any response which takes longer than 5 seconds will be marked as an error.

From line 2 and on we see the output from the executions. Note the amount of information available to us. The two highlighted lines (line 6 and 21) show the variables which have been discussed earlier. The first shows the connection time in various ways, such as the max, min and average along with the median and standard deviation. On line 21 we see the number of errors reported. No errors were reported during this execution.

A system administrator who wanted to use httperf to find the limits of their webserver would now repeat the command with a higher rate in order to stress the system more. The process would be repeated until errors start to occur. The challenge alongside this process is to adjust the total number of connection so as to allow the test to take roughly the same amount of time.

In conclusion we can identify two main challengers when using httperf: 
\begin{itemize}
	\item The rate needs to be incremented manually
	\item The number of connections needs to be adjusted in order to maintain benchmark length
\end{itemize}

A tool which uses httperf could provide some facility to solve these challenges.

\section*{Approach}

The operationalization asks for a script to be developed. The programming language used is Perl, which allows easy integration with the execution of commands and interaction with files.\\

When executed, the script would call httperf in a loop, always with an increasing rate. The number of connections should be adjusted, so that each test took the same amount of time. 
The script will be executed on the command line, so the next step is to identify the options which should be available to the user: 

\begin{itemize}
	\item The range of the connection rate: \Verb@-r min,max@
	\item The increment of the connection rate at each interval: \Verb@-i increment@
	\item The length of each test in seconds: \Verb@-l length@
	
	\item The target server and uri: \Verb@-s server@ and \Verb@-u uri@ 
	\item Additional flags, such as increased verbosity and debugging information: \Verb@-v@ and \Verb@-d@ respectively
\end{itemize}

The problem statement specifies that the tool should save the information for later analysis. In order to figure out the correct format of the output file, one should first think about how the analysis would take place. It is clear that a sort of history would beneficial, like a data file where one line represents a single execution. More executions would therefore correspond to more lines in the output file. The most important numbers therefore should be extracted from the httperf output and put into a single line with some practical delimiter such as a comma. This would make it easier to import the dataset into a tool for plotting.

The next question is, what would the X-axis, meaning the first column in the dataset, be? A timestamp could be useful in order to see when the test was conducted. However, that does not make much sense when considering it. The tests would take equal amount of time, so there is not much information gathered from displaying it relative to the time. That said, a timestamp might still be valuable data and could be included in the dataset. A better variable for the X-axis is the rate used for each test, as one would most likely want to view the performance \emph{relative} the rate of connections.

In addition to the rate, the actual rate should be displayed. The variable is printed on line 5 in the above example. The reason for including this variable is that one is not certain that the machine running httperf actually is capable of applying the desired pressure. It can be viewed as a form of control variable. In ideal conditions, the actual rate should correspond closely to the desired rate. The variables response time (average and median ) and timeout errors should give a good indication of webserver performance. Thus, the proposed output format is as follows: 


\begin{Verbatim}[commandchars=\\\{\},numbers=left,label=Proposed data output format]
con.rate, actual con.rate, average resp. time, median resp. time, errors, timestamp
\end{Verbatim}

\section{Result}

In this section the actual script is being presented. It was written based on the design in the previous section. 


The following options were implemented in the first version of the tool: 

\begin{Verbatim}[commandchars=\\\{\},numbers=left,label=All available options]
./hammer.pl -h
Options: 
-r min,max        The range of connection rates to use
-i increment      The increment at which to traverse the range
-l seconds        The desired length of each test
-s server         The target server
-u uri            The target uri
-c calls          The number of page requests per connection
-t seconds        The number of seconds to wait for a response
-o filename       Output filename. Use - for stdout
-v                Verbose, more output
-d                Debug, even more output
\end{Verbatim}



The script is executed on a client machine and will use httperf to execute a series of benchmarks towards a webserver. A typical execution will look like the following: 

\begin{Verbatim}[commandchars=\\\{\},numbers=left,label=hammer.pl execution example]
./hammer.pl -t 5 -i 10 -r 20,50 -s 128.39.73.239 -u '/read.php?file=load1024' -l 60 -c 5 -o out.dat
\end{Verbatim}

After running the benchmarks, the data is found in the output file. Here is an example output form the above execution. 

\begin{Verbatim}[commandchars=\\\{\},numbers=left,label=Output example: range 20 to 50]
20,20.0,21.4,21.5,0,1283423766
30,30.0,21.3,21.5,0,1283423826
40,40.0,21.6,21.5,0,1283423886
50,50.0,27.6,28.5,0,1283423946
\end{Verbatim}

From the uniformity of the data, we see that no errors or variation in the performance is visible. This might have the simple explanation that the saturation point is outside of the supplied range ( 20 to 50 connections per second). In order to investigate further, a second execution is done with a range from 60 to 90 connections per second. Only the resulting data is displayed: 

\begin{Verbatim}[commandchars=\\\{\},numbers=left,label=Output example: range 60 to 90]
60,56.2,2635.8,2426.5,211,1283424155
70,65.2,3345.1,2438.5,769,1283424219
80,73.5,3591.1,2437.5,1324,1283424283
90,82.5,3725.6,2565.5,1934,1283424348
\end{Verbatim}

From this output, we see a clear change in the data. If we combine the two outputs, the result becomes even clearer, but it can become a little difficult to identify the saturation point from the data directly. When used as input into a plotting tool (Plot for OS X in this case), one can see a clearer picture as to the performance degradation of the webserver. 

\begin{figure}[ht!]
\begin{centering}
\includegraphics[width=12cm]{output_example.pdf}
\caption{The performance of the tested webservice using the script and a supplied rate of 20 to 90 connections per second.}
\label{weboutput}
\end{centering}
\end{figure}

Shortly after 50 connections per second, we see a clear increase in the average as well as median reply time. The amount of errors increase too, however not so abrupt as one would have suggested. The stippled line along with the right-hand Y-axis show the actual connection rate. Based on the rather straight line, we can assume that the client running script indeed was able to pressure the webserver at almost the desired rate. Closer inspection reveals that the client side performance dropped slightly too. For example, at the desired rate of 90, only 82.5 connections per second were conducted. That is a performance loss of ca 8.3\%. 

\section{Analysis}

The script is written in Perl and functions as advertised. The main idea behind increasing the rate in a script is to facilitate faster identification of a servers saturation point. However, the results indicate that it may still be necessary for a user to execute several iterations of the script, if the indicated range actually does not contain the saturation point. A further modification to the script may solve this in that only a start rate and increment is given as options. The tool could then continue to increase the rate automatically until the saturation point is identified or the actual connection rate would drop considerably below the desired rate, meaning the client is unable to continue to apply more pressure. Identification of the saturation could be based on the number of timeouts perceived too, using a fixed threshold such as 5\% timeouts.

\subsection{New version}

A new version of the tool was developed with these modifications. It now has an additional user option \Verb@-R startrate@, which only sets the starting point. This option is mutually exclusive from the \Verb@-r min,max@ option. The script will continue to increase the rate until the amount of timeout errors surpasses 5\% of the total requests. At that point, the script will finish.\\

Here is an example of the script being executed with the new option: 


\begin{Verbatim}[commandchars=\\\{\},numbers=left,label=hammer.pl execution example with new option]
./hammer.pl -t 5 -i 10 -R 20 -s 128.39.73.239 -u '/read.php?file=load1024' -l 60 -c 5 -o out.dat
\end{Verbatim}

The result is as follows: 
\begin{Verbatim}[commandchars=\\\{\},numbers=left,label=hammer.pl execution example with new option]
20,20.0,21.8,21.5,0,1283426590
30,30.0,21.5,21.5,0,1283426650
40,40.0,21.4,21.5,0,1283426710
50,50.0,27.3,27.5,0,1283426770
50,50.0,27.6,28.5,0,1283427593
60,56.5,2399.0,2386.5,179,1283427653
70,65.1,3300.0,2421.5,748,1283427717
80,73.5,3698.7,2521.5,1329,1283427781
\end{Verbatim}


From the result, we see that the script continues until the rate of 80 (see line 8), at which point the saturation point has already been passed. To be precise: the desired rate was 80 connections per second with 5 requests per connection over 60 seconds (see the supplied options). The total number of requests, therefore, is: 

\[
	Requests_{total} = rate * requests * time
\] 
\[
	Requests_{total} = 80 * 5 * 60\\
\]
\[
	Requests_{total} = 24000\\
\]

With 1329 reported errors, we cross the threshold of 1200 ( 24000 * 0.05 ).\\


The thresold found in this version is similar to earlier attempts, which can be confirmed when looking at Figure \ref{weboutput}. The result indicates that with this additional functionality, the saturation point could be identified with only a single execution of the command, instead of two, using the same initial assumptions of the user.
 
\section{Discussion and conclusion}

The aim of the tool was to increase the rate and write the output to a file. In addition to this, the tool has simplified usage by adding some features which enables one to automatically identify a servers saturation point. One could argue, however, that by adding these features, the overall complexity has led to a tool which is more complicated than originally asked for. The added complexity takes the form of more user options and perhaps more documentation on how to use it. In this case, I'd argue that the benefits outweigh the drawbacks of more options. It leads to a more accurate tool than just allowing a certain range of rates. After all, the goal is to identify a servers saturation point.\\

More additions could be added to this tool in order to address issues such as repeated experiments in order to gage the predictability of the results. This is still left to the user to do manually. A second tool which was built on top of this one could re-run the experiments and do more advanced calculations. The format of the filesystem is suitable for third-party analysis.\\

More variables from httperf could be included into the output file. Right now, only four variables are listed. This should be a trivial addition to this script which would broaden the possibilities for analysis.\\

The script will function on any Linux system with Perl and httperf installed, which should facilitate adoption. Any webserver can be the target of httperf.\\

In many cases, a single machine will be unable to hammer a server with a sufficient rate in order to reach its saturation point. This goes especially for larger setups with more than one webserver and load balancing. In that case, this script should be transformed into a distributed framework of multiple clients which can launch a coordinated benchmark against a target and then collect the results for a summary. The core component of the script would still be the same as in this tool, but with the addition of a socket-based communication framework and a centralized manager.\\

\subsection{Conclusion}

In summary, the task has been completed in the following way: 
\begin{itemize}
	\item Two main challenges have been solved when using httperf improves the usability.
	\item A better output format for the inclusion into plotting tools improves the analysis by easing import into plotting tools.
\end{itemize}
Further improvements have been suggested, which may be picked up by future work. 

\section{Appendix: hammer.pl}

\begin{Verbatim}[numbers=left,label=The hammer.pl script]
#!/usr/bin/perl

# Needed packages
use Getopt::Std;
use strict "vars";
use POSIX;

# Global variables
my $VERBOSE = 0;
my $DEBUG = 0;

#####################
# handle flags and arguments
# Example: c == "-c", c: == "-c argument"
my $opt_string = 'hvdo:u:s:t:r:R:i:c:l:';
getopts( "$opt_string", \my %opt ) or usage() and exit 1;

# print help message if -h is invoked
if ( $opt{'h'} ){
    usage();
    exit 0;
}

$VERBOSE = 1 if $opt{'v'};
$DEBUG = 1 if $opt{'d'};


# main program content

my $server = $opt{'s'};
my $uri = $opt{'u'};
my $timeout = $opt{'t'};
my $output = $opt{'o'};
my $range = $opt{'r'};
my $increment = $opt{'i'};
my $count = $opt{'c'};
my $length = $opt{'l'};
my $startrate = $opt{'R'};

if ( $startrate and $range ){
    print "You can not use -R and -r in the same execution\n";
    usage();
    exit 1;
}

( my $low_rate, my $high_rate ) = split /,/,$range;

verbose("Low rate: $low_rate, high rate: $high_rate\n");
# handle the -R option
if ( $startrate ){
    $low_rate = $startrate unless $low_rate;
    $high_rate = 10000;
}

# blank the data output file
if ( $output ne "-" ){
         open(OUT,">$output");
         close(OUT);
}
# The actual loop performing the tests
for ( my $rate = $low_rate; $rate <= $high_rate ; $rate += $increment ){

    my $conns = $rate * $length;
    my $timestamp = time;
    my $command = "httperf --server $server --uri \'$uri\' --num-conn $conns --num-call $count --rate $rate --timeout $timeout";
    verbose("Running command: $command\n");
    open(HTTPERF,"$command |");
    my $cavg;
    my $cmed;
    my $cdev;
    my $timeo;
    my $crate;
    while ( my $htp = <HTTPERF> ){
        verbose("$htp");
        if ( $htp =~ /Connection time .*avg (\d+\.\d) .* median (\d+\.\d) stddev (\d+\.\d)/ ){

            $cavg = $1;
            $cmed = $2;
            $cdev = $3;

        } elsif ( $htp =~ /Errors: .* client-timo (\d+) / ){
            $timeo = $1;
        } elsif ( $htp =~ /Connection rate: (\d+.\d)/ ){
            $crate = $1;
        }

    }

    if ( $output eq "-" ){
         print "$rate,$crate,$cavg,$cmed,$timeo,$timestamp\n";
     } else {
         open(OUT,">>$output");
         print OUT "$rate,$crate,$cavg,$cmed,$timeo,$timestamp\n";
         close(OUT);
     }
    if ( $startrate ){
        my $error_ratio = ( $timeo / ( $rate * $count * $length ) );
        verbose("error ratio: $error_ratio\n");
        if ( $error_ratio >= 0.05 ){
            verbose("Over 10% errors, saturation point found\n");
            $high_rate = 1;
        }
    }
}

sub verbose {
    print $_[0] if ( $VERBOSE or $DEBUG );
}

sub debug {
    print $_[0] if ( $DEBUG );
}

sub usage {

    print "Options: \n";
    print "-r min,max        The range of connection rates to use\n";
    print "-i increment      The increment at which to traverse the range\n";
    print "-l seconds        The desired length of each test\n";
    print "-s server         The target server\n";
    print "-u uri            The target uri\n";
    print "-c calls          The number of page requests per connection\n";
    print "-t seconds        The number of seconds to wait for a response\n";
    print "-o filename       Output filename. Use - for stdout\n";
    print "-v                Verbose, more output\n";
    print "-d                Debug, even more output\n";
}	
\end{Verbatim}
